---
title:  'Curso Modelos Lineales'
subtitle: <h1>Magister en Estadística Aplicada</h1>
author:
- UNC 2022
date: "Guia de Actividades I"
output: 
  rmdformats::readthedown
code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, background = NA, results='asis', warning=FALSE)
```

## Ejercicio 1
```{r echo=FALSE}
library("mat2tex")
U=matrix(c(0,0,0),nrow=3,byrow=TRUE)
W=matrix(c(3,1,0,1,2,1,0,1,4),nrow=3,byrow=TRUE)
```

Sea $X \sim N_3(0,\sum)$ con: $\mu$=
```{r echo=FALSE,results='asis'}
"$" %_% xm(U, mtype = "bm")  %_% "$"
```
y $\sum =$ 
```{r echo=FALSE,results='asis'}
"$" %_% xm(W, mtype = "bm") %_% "$"
```

#### a. Plantear $f(X)$

Dada un vector aleatorio $\mathbf{X}$ con $n$ variables aleatorias, y con vector de esperanzas $\boldsymbol{\mu}$ y matriz de covarianzas $\boldsymbol{\Sigma}$, la función de densidad conjunta viene dada por la forma general

$$
\begin{aligned}
f(\mathbf{X}) = (2\pi)^{-\frac {n} {2}} |\Sigma|^{-\frac {1} {2}} 
e^{-\frac {1} {2} \big[(\mathbf{X} - \boldsymbol{\mu})' \Sigma^{-1} (\mathbf{X} - \boldsymbol{\mu})\big]}   
\end{aligned}
$$
 
Considerando que $n=3$ y que $\boldsymbol{\mu} = \begin{pmatrix} 0 & 0 & 0 \end{pmatrix}'$ entonces

$$
\begin{aligned}
f(\mathbf{X}) = (2\pi)^{-\frac {3} {2}} |\Sigma|^{-\frac {1} {2}} 
e^{-\frac {1} {2} \big[\mathbf{X} ' \Sigma^{-1} \mathbf{X} \big]}   
\end{aligned}
$$

#### b. Encontrar las distribuciones marginales de $X_1$ y $X_3$.

Para hallar la distribución marginal de un sub-vector $X'^{(1)}$ se aplica el teorema de distribución de combinaciones lineales de vectores aleatorios, escogiendo apropiadamente la matriz **C**. Este teorema asegura toda combinación lineal de un vector normal multivariado sigue una distribución normal multivariada.

Si se toma $C = \begin{bmatrix} I_k ; 0_{k \times (n-k)} \end{bmatrix}$ y $c=0$ se tiene que $CX+c = X'^{(1)}$ y, luego, la distribución resultante será 

$$
\begin{aligned}
N(C\mu+c, C \Sigma C') &= N(\mu_1, \Sigma_{11}) \quad \text{donde} \\
\mu_1&=E(X'^{(1)})\\
\Sigma_{11}  &\text{ es el bloque de varianzas y covarianzas correspondiente a } X'^{(1)} 
\end{aligned}
$$

Se expresa $X_1$ como combinación lineal del vector $X$. Sea $X' = \begin{bmatrix} X_1 & X_2 & X_3 \end{bmatrix} = \begin{bmatrix} [X_1] & [X_2 & X_3] \end{bmatrix} = \begin{bmatrix} X'^{(1)} & X'^{(2)} \end{bmatrix}$

Entonces en la forma general

$$
\begin{aligned}
X'^{(1)} = CX+c = \begin{bmatrix} I_k & 0_{k \times (n-k)} \end{bmatrix} X + 0 \\
X'^{(1)}  \sim N(\mu_1 = C\mu+c, \Sigma_{11}=C \Sigma C')
\end{aligned}
$$
Luego en la forma particular

$$
\begin{aligned}
X'^{(1)} &= CX+c \\
 &= \begin{bmatrix} I_1 & 0_{1 \times (3-1)} \end{bmatrix} X + 0  \\
 &= \begin{bmatrix} 1 & 0 & 0\end{bmatrix} \begin{bmatrix} X_1 \\ X_2 \\ X_3 \end{bmatrix} + 0 \\
 &= X_1
\end{aligned}
$$

Finalmente

$$
\begin{aligned}
X_1 &\sim N(
  \mu_1 = \begin{bmatrix} 1 & 0 & 0\end{bmatrix} \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} + 0 , 
  \Sigma_{11} = \begin{bmatrix} 1 & 0 & 0\end{bmatrix} 
                \begin{bmatrix} 3 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 4 \end{bmatrix} 
                \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} = 3 ) \\
X_1 &\sim N(\mu_1 = 0, \Sigma_{11} = \begin{bmatrix} 3 \end{bmatrix}) 
\end{aligned}
$$

Del mismo modo para $X_3$

$$
\begin{aligned}
X_3 &\sim N(
  \mu_1 = \begin{bmatrix} 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} + 0 , 
  \Sigma_{11} = \begin{bmatrix} 0 & 0 & 1\end{bmatrix} 
                \begin{bmatrix} 3 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 4 \end{bmatrix} 
                \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} = 4 ) \\
X_3 &\sim N(\mu_1 = 0, \Sigma_{11} = \begin{bmatrix} 4 \end{bmatrix}) 
\end{aligned}
$$

#### c. Encontrar la distribución conjunta de $(X_1,X_2)$ y $(X_1,X_3)$.

Se expresa $(X_1, X_2)$ como combinación lineal del vector $X$. 
Sea 

$$
\begin{aligned}
X' = \begin{bmatrix} X_1 & X_2 & X_3 \end{bmatrix} = \begin{bmatrix} [X_1 & X_2] & [X_3] \end{bmatrix} = \begin{bmatrix} X'^{(1)} & X'^{(2)} \end{bmatrix}
\end{aligned}
$$

Entonces en la forma general

$$
\begin{aligned}
X'^{(1)} = CX+c = \begin{bmatrix} I_k & 0_{k \times (n-k)} \end{bmatrix} X + 0 \\
X'^{(1)}  \sim N(\mu_1 = C\mu+c, \Sigma_{11}=C \Sigma C')
\end{aligned}
$$
Luego en la forma particular

$$
\begin{aligned}
X'^{(1)} &= CX+c \\
 &= \begin{bmatrix} I_2 & 0_{2 \times (3-2)} \end{bmatrix} X + 0  \\
 &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} 
    \begin{bmatrix} X_1 \\ X_2 \\ X_3 \end{bmatrix} + 
    \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\
 &= \begin{bmatrix} X_1 \\ X_2 \end{bmatrix}
\end{aligned}
$$

Finalmente

$$
\begin{aligned}
\begin{bmatrix} X_1 \\ X_2 \end{bmatrix} &\sim N_2(
  \mu_1 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} 
          \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} + 
          \begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
  \Sigma_{11} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} 
                \begin{bmatrix} 3 & 1 & 0 \\ 1 & 2 & 1 \\ 0 & 1 & 4 \end{bmatrix} 
                \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}  ) \\
\begin{bmatrix} X_1 \\ X_2 \end{bmatrix} &\sim N_2(\mu_1 =  \begin{bmatrix} 0 \\ 0 \end{bmatrix} , \Sigma_{11} = 
                \begin{bmatrix} 3 & 1 \\ 1 & 2 \end{bmatrix}) 
\end{aligned}
$$



El caso de $(X_1, X_3)$ es similar al anterior, pero previamente debe reordenarse los elementos de $X$.
Sea 

$$
\begin{aligned}
X' = \begin{bmatrix} X_1 & X_3 & X_2 \end{bmatrix} = \begin{bmatrix} [X_1 & X_3] & [X_2] \end{bmatrix} = \begin{bmatrix} X'^{(1)} & X'^{(2)} \end{bmatrix}
\end{aligned}
$$

Así en la forma particular

$$
\begin{aligned}
X'^{(1)} &= CX+c \\
 &= \begin{bmatrix} I_2 & 0_{2 \times (3-2)} \end{bmatrix} X + 0  \\
 &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} 
    \begin{bmatrix} X_1 \\ X_3 \\ X_2 \end{bmatrix} + 
    \begin{bmatrix} 0 \\ 0 \end{bmatrix} \\
 &= \begin{bmatrix} X_1 \\ X_3 \end{bmatrix}
\end{aligned}
$$

Finalmente

$$
\begin{aligned}
\begin{bmatrix} X_1 \\ X_3 \end{bmatrix} &\sim N_2(
  \mu_1 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} 
          \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} + 
          \begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
  \Sigma_{11} = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \end{bmatrix} 
                \begin{bmatrix} 3 & 0 & 1 \\ 0 & 4 & 1 \\ 1 & 1 & 2 \end{bmatrix} 
                \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}  ) \\
\begin{bmatrix} X_1 \\ X_3 \end{bmatrix} &\sim N_2(\mu_1 =  \begin{bmatrix} 0 \\ 0 \end{bmatrix} , \Sigma_{11} = 
                \begin{bmatrix} 3 & 0 \\ 0 & 4 \end{bmatrix}) 
\end{aligned}
$$



#### d. Encontrar la distribución condicional de $X_2 \mid X_1=x_1, X_3=x_3$ y $(X_1,X_3 \mid X_2=x_2)$.

 - Para $X_2 \mid X_1=x_1, X_3=x_3$

Sea 

$$
\begin{aligned}
X' = \begin{bmatrix} X_2 & X_1 & X_3 \end{bmatrix} = \begin{bmatrix} [X_2] & [X_1 & X_3] \end{bmatrix} = \begin{bmatrix} X'^{(1)} & X'^{(2)} \end{bmatrix}
\end{aligned}
$$

Entonces en la forma general

$$
\begin{aligned}
X'^{(1)} / X'^{(2)} \sim N_k(\mu_{1.2} = \mu^1 + \Sigma_{12}\Sigma_{22}^{-1}(x-\mu)^{(2)}, 
                             \Sigma_{11.2} = \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} )
\end{aligned}
$$

Luego en la forma particular

$$
\begin{aligned}
  \Sigma &= \begin{bmatrix} 2 & 1 & 1 \\ 1 & 3 & 0 \\ 1 & 0 & 4 \end{bmatrix} =
            \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}  \end{bmatrix} \\
  \mu_{1.2} &= \mu^1 + \Sigma_{12}\Sigma_{22}^{-1}(x-\mu)^{(2)} =
               \begin{bmatrix} 0  \end{bmatrix} +
               \begin{bmatrix} 1 & 1 \end{bmatrix} 
               \begin{bmatrix} 3 & 0 \\ 0 & 4  \end{bmatrix}^{-1}
               \begin{bmatrix} x_1  - 0 \\ x_3  - 0 \end{bmatrix} \\
            &= \begin{bmatrix} 1 & 1  \end{bmatrix}  
               \begin{bmatrix} \frac {1} {3} & 0 \\ 0 & \frac {1} {4}  \end{bmatrix} 
               \begin{bmatrix} x_1 \\ x_3 \end{bmatrix} = 
               \begin{bmatrix} \frac {x_1} {3}  + \frac {x_3} {4}  \end{bmatrix} \\
  \Sigma_{11.2} &= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} =      
                   \begin{bmatrix} 2 \end{bmatrix} - 
                   \begin{bmatrix} 1 & 1  \end{bmatrix} 
                   \begin{bmatrix} 3 & 0 \\ 0 & 4  \end{bmatrix}^{-1}
                   \begin{bmatrix} 1 \\ 1  \end{bmatrix} \\
                &= \begin{bmatrix} 2 \end{bmatrix} - 
                   \begin{bmatrix} 1 & 1 \end{bmatrix} 
                   \begin{bmatrix} \frac {1} {3} & 0 \\ 0 & \frac {1} {4}  \end{bmatrix} 
                   \begin{bmatrix} 1 \\ 1  \end{bmatrix} =
                   \begin{bmatrix} 2 \end{bmatrix} - 
                   \begin{bmatrix} \frac {1} {3}  + \frac {1} {4} \end{bmatrix} \\
                &= \begin{bmatrix} \frac {17} {12} \end{bmatrix}
\end{aligned}
$$

Finalmente

$$
\begin{aligned}
  X_2 | X_1 = x_1 \land X_3 = x_3 &\sim N_1(
  \mu_{1.2} = \begin{bmatrix} \frac {x_1} {3}  + \frac {x_3} {4}  \end{bmatrix}, 
  \Sigma_{11.2} = \begin{bmatrix} \frac {17} {12} \end{bmatrix}  ) 
\end{aligned}
$$              
A continuación, trabajamos con datos en R

```{r echo=TRUE}
# Matriz de esperanzas
mu <- matrix(c(0,0,0),nrow=3,byrow=TRUE)

# Matriz de varianzas y covarianzas
S <- matrix(c(3,1,0,1,2,1,0,1,4),nrow=3,byrow=TRUE)

# Particion en Y1=[X2] y Y2=[X1, X3]
Y1 <- c(2)
Y2 <- c(1,3)

mu_Y1 <- mu[Y1] 
mu_Y2 <- mu[Y2] 

S_11 <- S[Y1,Y1]
S_12 <- S[Y1,Y2]
S_21 <- S[Y2,Y1]
S_22 <- S[Y2,Y2]

# Esperanza condicional
mu_Y1_Y2 <- round(mu_Y1 + S_12 %*% solve(S_22), 3)
mu_Y1_Y2

# Varianza condicional
S_Y1_Y2 <- round(S_11 - S_12 %*% solve(S_22) %*% S_21, 3)
S_Y1_Y2
```

 - Para $X_1, X_3 \mid X_2=x_2$

Sea 

$$
\begin{aligned}
X' = \begin{bmatrix} X_1 & X_3 & X_2 \end{bmatrix} = \begin{bmatrix} [X_1 & X_3] & [X_2] \end{bmatrix} = \begin{bmatrix} X'^{(1)} & X'^{(2)} \end{bmatrix}
\end{aligned}
$$

En la forma particular

$$
\begin{aligned}
  \Sigma &= \begin{bmatrix} 3 & 0 & 1 \\ 0 & 4 & 1 \\ 1 & 1 & 2 \end{bmatrix} =
            \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22}  \end{bmatrix} \\
  \mu_{1.2} &= \mu^1 + \Sigma_{12}\Sigma_{22}^{-1}(x-\mu)^{(2)} =
               \begin{bmatrix} 0 \\ 0 \end{bmatrix} +
               \begin{bmatrix} 1 \\ 1 \end{bmatrix} 
               \begin{bmatrix} 2  \end{bmatrix}^{-1}
               \begin{bmatrix} x_2  - 0 \end{bmatrix} \\
            &= \begin{bmatrix} 1 \\ 1  \end{bmatrix}  
               \begin{bmatrix} \frac {x_2} {2} \end{bmatrix} = 
               \begin{bmatrix} \frac {x_2} {2} \\ \frac {x_2} {2} \end{bmatrix} \\
  \Sigma_{11.2} &= \Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \Sigma_{21} =      
                   \begin{bmatrix} 3 & 0 \\ 0 & 4  \end{bmatrix} - 
                   \begin{bmatrix} 1 \\ 1  \end{bmatrix} 
                   \begin{bmatrix} 2 \end{bmatrix}^{-1}
                   \begin{bmatrix} 1 & 1  \end{bmatrix} \\
                &= \begin{bmatrix} 3 & 0 \\ 0 & 4 \end{bmatrix} - 
                   \begin{bmatrix} 1 \\ 1 \end{bmatrix} 
                   \begin{bmatrix} \frac {1} {2} & \frac {1} {2}  \end{bmatrix} \\
                &= \begin{bmatrix} 3 & 0 \\ 0 & 4 \end{bmatrix} - 
                   \begin{bmatrix} \frac {1} {2} & \frac {1} {2} \\ \frac {1} {2} & \frac {1} {2}  \end{bmatrix} =
                   \begin{bmatrix} \frac {5} {2} & -\frac {1} {2} \\ -\frac {1} {2} & \frac {7} {2} \end{bmatrix}
\end{aligned}
$$

Finalmente

$$
\begin{aligned}
  X_1, X_3 | X_2 = x_2 &\sim N_2(
    \mu_{1.2} = \begin{bmatrix} \frac {x_2} {2} \\ \frac {x_2} {2}  \end{bmatrix}, 
    \Sigma_{11.2} = \begin{bmatrix} \frac {5} {2} & -\frac {1} {2} \\ -\frac {1} {2} & \frac {7} {2} \end{bmatrix}  ) 
\end{aligned}
$$              
A continuación, trabajamos con datos en R

```{r echo=TRUE}
# Matriz de esperanzas
mu <- matrix(c(0,0,0),nrow=3,byrow=TRUE)

# Matriz de varianzas y covarianzas
S <- matrix(c(3,1,0,1,2,1,0,1,4),nrow=3,byrow=TRUE)

# Particion en Y1=[X2] y Y2=[X1, X3]
Y1 <- c(1,3)
Y2 <- c(2)

mu_Y1 <- mu[Y1] 
mu_Y2 <- mu[Y2] 

S_11 <- S[Y1,Y1]
S_12 <- S[Y1,Y2]
S_21 <- S[Y2,Y1]
S_22 <- S[Y2,Y2]

# Esperanza condicional
mu_Y1_Y2 <- round(mu_Y1 + S_12 %*% solve(S_22), 3)
mu_Y1_Y2

# Varianza condicional
S_Y1_Y2 <- round(S_11 - S_12 %*% solve(S_22) %*% S_21, 3)
S_Y1_Y2
```



#### e. Encontrar la matriz de correlación.

Se tiene que 

$$
\begin{aligned}
  \Pi = D^{- \frac {1} {2} } \Sigma D^{- \frac {1} {2} } \quad \quad \quad \text{ donde } D=diag(\Sigma)
\end{aligned}
$$              

Así, 

$$
\begin{aligned}
  \Sigma &= \begin{bmatrix} 3 & 0 & 1 \\ 0 & 4 & 1 \\ 1 & 1 & 2 \end{bmatrix} \\
  D &= \begin{bmatrix} 3 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 2 \end{bmatrix} \implies
  D^{- \frac {1} {2}} = \begin{bmatrix} 
                          \frac {1} {\sqrt 3} & 0 & 0 \\ 
                          0 & \frac {1} {\sqrt 4} & 0 \\ 
                          0 & 0 & \frac {1} {\sqrt 2} 
                        \end{bmatrix}\\
\end{aligned}
$$

Luego, 

$$
\begin{aligned}
  \Pi &= \begin{bmatrix} 
           \frac {1} {\sqrt 3} & 0 & 0 \\ 
           0 & \frac {1} {\sqrt 4} & 0 \\ 
           0 & 0 & \frac {1} {\sqrt 2} 
         \end{bmatrix}
         \begin{bmatrix} 3 & 0 & 1 \\ 0 & 4 & 1 \\ 1 & 1 & 2 \end{bmatrix} 
         \begin{bmatrix} 
           \frac {1} {\sqrt 3} & 0 & 0 \\ 
           0 & \frac {1} {\sqrt 4} & 0 \\ 
           0 & 0 & \frac {1} {\sqrt 2} 
         \end{bmatrix}
      &= \begin{bmatrix} 1 & 0.4082 & 0 \\ 0.4082 & 1 & 0.3536 \\ 0 & 0.3536 & 1 \end{bmatrix} 
\end{aligned}
$$
A continuación, trabajamos con datos en R

```{r echo=TRUE}
# Matriz de varianzas y covarianzas
S <- matrix(c(3,1,0,1,2,1,0,1,4),nrow=3,byrow=TRUE)

# Matriz D
D <- diag(diag(S))

# Matriz de correlacion
R <- solve(sqrt(D)) %*% S %*% solve(sqrt(D))
```

```{r echo=FALSE}

"$" %_% xm(R ,3, mtype = "bm")  %_% "$"

```


#### f. Encontrar $\rho_{{13}\mid 2} ,\rho_{{12}\mid 3}$.

 - Para $\rho_{{13}\mid 2}$

Sea 

$$
\begin{aligned}
X' = \begin{bmatrix} X_1 & X_3 & X_2 \end{bmatrix} = \begin{bmatrix} [X_1 & X_3] & [X_2] \end{bmatrix} = \begin{bmatrix} X'^{(1)} & X'^{(2)} \end{bmatrix}
\end{aligned}
$$

Se tiene que 

$$
\begin{aligned}
  \Pi_{[X_1 \: X_3] \; | \; [X_2]} = D^{- \frac {1} {2} } \: \Sigma_{11.2} \: D^{- \frac {1} {2} } \qquad \qquad 
                                     \text{ donde } D=diag(\Sigma_{11.2}) 
                                     \text{ y } \Sigma_{11.2} = \begin{bmatrix} 
                                                                   \frac {5} {2} & -\frac {1} {2} \\ 
                                                                  -\frac {1} {2} &  \frac {7} {2} 
                                                                \end{bmatrix} 
\end{aligned}
$$  

Luego, 

$$
\begin{aligned}
  \Pi_{[X_1 \: X_3] \; | \; [X_2]} &= 
         \begin{bmatrix} \frac {5} {2} & 0 \\ 0 &  \frac {7} {2} \end{bmatrix}^{- \frac {1} {2} }  
         \begin{bmatrix} 
             \frac {5} {2} & -\frac {1} {2} \\ 
            -\frac {1} {2} &  \frac {7} {2} 
         \end{bmatrix}          
         \begin{bmatrix} \frac {5} {2} & 0 \\ 0 &  \frac {7} {2} \end{bmatrix}^{- \frac {1} {2} }  
      = \begin{bmatrix} 1 & -0.1690  \\ -0.1690 & 1  \end{bmatrix} 
\end{aligned}
$$

Finalmente,

$$
\begin{aligned}
  \rho_{{13}\mid 2} = -0,1690
\end{aligned}
$$

A continuación, trabajamos con datos en R

```{r echo=TRUE}
# Matriz de varianzas y covarianzas
S <- matrix(c(3,1,0,1,2,1,0,1,4),nrow=3,byrow=TRUE)

# Particion en Y1=[X2] y Y2=[X1, X3]
Y1 <- c(1,3)
Y2 <- c(2)

S_11 <- S[Y1,Y1]
S_12 <- S[Y1,Y2]
S_21 <- S[Y2,Y1]
S_22 <- S[Y2,Y2]

# Varianza condicional
S_Y1_Y2 <- round(S_11 - S_12 %*% solve(S_22) %*% S_21, 3)

# Matriz D
D_Y1_Y2 <- diag(diag(S_Y1_Y2))

# Matriz de correlacion
R_Y1_Y2 <- solve(sqrt(D_Y1_Y2)) %*% S_Y1_Y2 %*% solve(sqrt(D_Y1_Y2))
```

```{r echo=FALSE}

"$" %_% xm(R_Y1_Y2 ,3, mtype = "bm")  %_% "$"

```


 - Para $\rho_{{12} \mid 3}$

Sea 

$$
\begin{aligned}
X' = \begin{bmatrix} X_1 & X_2 & X_3 \end{bmatrix} = \begin{bmatrix} [X_1 & X_2] & [X_3] \end{bmatrix} = \begin{bmatrix} X'^{(1)} & X'^{(2)} \end{bmatrix}
\end{aligned}
$$

Se tiene que 

$$
\begin{aligned}
  \Pi_{[X_1 \: X_2] \; | \; [X_3]} = D^{- \frac {1} {2} } \: \Sigma_{11.2} \: D^{- \frac {1} {2} } \qquad \qquad 
                                     \text{ donde } D=diag(\Sigma_{11.2}) 
                                     \text{ y } \Sigma_{11.2} = \begin{bmatrix} 
                                                                   3 & 1 \\ 
                                                                   1 &  \frac {7} {4} 
                                                                \end{bmatrix} 
\end{aligned}
$$  

Luego, 

$$
\begin{aligned}
  \Pi_{[X_1 \: X_2] \; | \; [X_3]} &= 
         \begin{bmatrix} 3 & 0 \\ 0 &  \frac {7} {4} \end{bmatrix}^{- \frac {1} {2} }  
         \begin{bmatrix} 
             3 & 1 \\ 
             1 &  \frac {7} {4} 
         \end{bmatrix}          
         \begin{bmatrix} 3 & 0 \\ 0 &  \frac {7} {4} \end{bmatrix}^{- \frac {1} {2} }  
      = \begin{bmatrix} 1 & 0.436 \\ 0.436 & 1  \end{bmatrix} 
\end{aligned}
$$

Finalmente,

$$
\begin{aligned}
  \rho_{{12}\mid 3} = 0.436
\end{aligned}
$$

A continuación, trabajamos con datos en R

```{r echo=TRUE}
# Matriz de varianzas y covarianzas
S <- matrix(c(3,1,0,1,2,1,0,1,4),nrow=3,byrow=TRUE)

# Particion en Y1=[X2] y Y2=[X1, X3]
Y1 <- c(1,2)
Y2 <- c(3)

S_11 <- S[Y1,Y1]
S_12 <- S[Y1,Y2]
S_21 <- S[Y2,Y1]
S_22 <- S[Y2,Y2]

# Varianza condicional
S_Y1_Y2 <- round(S_11 - S_12 %*% solve(S_22) %*% S_21, 3)

# Matriz D
D_Y1_Y2 <- diag(diag(S_Y1_Y2))

# Matriz de correlacion
R_Y1_Y2 <- solve(sqrt(D_Y1_Y2)) %*% S_Y1_Y2 %*% solve(sqrt(D_Y1_Y2))
```

```{r echo=FALSE}

"$" %_% xm(R_Y1_Y2 ,3, mtype = "bm")  %_% "$"

```


#### g. Encontrar la distribución de $Z=3x_1 - 2x_2 -11$.


#### h. Encontrar la covarianza entre $Z_1$ y $Z_2$ donde $Z_1=2 X_1 -3 X_2+X_3 -3$ Y $Z_2=3 X_3+2$.

## Ejercicio 2

En el archivo *calefacción* se muestran datos correspondientes a una población de 20 inmuebles ubicados en cierta zona residencial. Se consideran tres variables que están relacionadas con el costo de calefacción de la vivienda: la temperatura exterior media diaria ($X_1$), el número de cm. de aislamiento térmico de las paredes ($X_2$) y la antiguedad del calefactor ($X_3$).
a. Obtener el vector de medias y la matriz de varianzas y covarianzas del vector $X´= (X_1 X_2 X_3)$.
b. Obtener la matriz de correlación.
c. Obtener la matriz de correlación parcial de $X^{(1)}= (X_1 X_2$ dado $X^{(2)}=X_3$.

## Ejercicio 3

```{r echo=FALSE}
UU<-matrix(c(2,1,4),ncol =1,byrow=TRUE)
WW<-matrix(c(2,-1,0,-1,4,0,0,0,1),ncol =3,byrow=FALSE)
```

Sea $X \sim N_3(\mu,\sum)$ con: $\mu$=
```{r echo=FALSE,results='asis'}
"$" %_% xm(UU, mtype = "bm")  %_% "$"
```
y $\sum =$ 
```{r echo=FALSE,results='asis'}
"$" %_% xm(WW, mtype = "bm") %_% "$"
```

a. Encontrar la distribución conjunta de $X_1$ y $X_2$.
b. Encontrar la distribución condicional de $X_1=x_1, X_2=x_2 \mid X_3=x_3$, la esperanza y varianza.

## Ejercicio 4

```{r echo=FALSE}
UUU<-matrix(c(0,0,0),ncol =1,byrow=TRUE)
WWW<- matrix(c(7,2,1,2,7,-1,1,-1,4),3,3) 
```

Sea $X \sim N_3(0,\sum)$, encontrar la transformación $CX$ tal que la matriz de covarianzas de la variable resultante sea:$\sum =$ 
```{r echo=FALSE,results='asis'}
"$" %_% xm(WWW, mtype = "bm") %_% "$"
```
. (Ayuda. Pensar en descomposición espectral o descomposición de Cholesky)

## Ejercicio 5

```{r echo=FALSE}
UUUU<-matrix(c(1,2),ncol =1,byrow=TRUE)
WWWW<- matrix(c(3,1,1,3),2,2) 
```

Dada $X\sim N_2($
```{r echo=FALSE,results='asis'}
"$" %_% xm(UUUU, mtype = "bm")  %_% "$"
```
, 
```{r echo=FALSE,results='asis'}
"$" %_% xm(WWWW, mtype = "bm") %_% "$"
```
$)$ encontrar la transformación $Y=TX+c$ tal que $Y\sim N_3(0,I)$


## Ejercicio 6

En el ejercicio anterior la función de densidad multivariada está dada por:
$f(x)=(2\pi)^{-1} \mid\sum\mid^{-1/2} e^{-1/2[(x-\mu)´\sum^{-1} (x-\mu)]}$ donde $Q=(x-\mu)´\sum^{-1}(x-\mu)$ es una forma cuadrática que mide el cuadrado de la distancia entre cada punto del plano y el vector de medias $\mu$. Encontrar la distribución de dicha forma cuadrática.

## Ejercicio 7
```{r echo=FALSE,results='asis'}
B<- matrix(c(2,-1,-1,0,0,1),2,3) 
A<- matrix(c(1/6,1/3,1/6,1/3,2/3,1/3,1/6,1/3,1/6),3,3) 
```

Dado $Y\sim N_3(\mu,I)$ establecer si el vector $BY$ es independiente de la forma cuadrática $Y´AY$ donde: $B=$
```{r echo=FALSE,results='asis'}
"$" %_% xm(B, mtype = "bm") %_% "$"
```
y $A=$
```{r echo=FALSE,results='asis'}
"$" %_% xm(A, mtype = "bm") %_% "$"
```

## Ejercicio 8

```{r echo=FALSE,results='asis'}
A<- matrix(c(2,-1,-1,1),2,2)
B<- matrix(c(0.5,-1,0.5,1),2,2)
W5<- matrix(c(1,1,1,2),2,2)
U5<-matrix(c(1,3),2,1)
```


Dado $Y\sim N_2($
```{r echo=FALSE,results='asis'}
"$" %_% xm(U5, mtype = "bm")  %_% "$"
```
, 
```{r echo=FALSE,results='asis'}
"$" %_% xm(W5, mtype = "bm") %_% "$"
```
$)$, con: $B=$
```{r echo=FALSE,results='asis'}
"$" %_% xm(B, mtype = "bm") %_% "$"
```
y $A=$
```{r echo=FALSE,results='asis'}
"$" %_% xm(A, mtype = "bm") %_% "$"
```

a. Probar si la forma linear $BY$ es independiente de la forma cuadrática $Y´AY$.
b. Determinar la distribución de la forma cuadrática $Y´AY$.

## Ejercicio 9

```{r echo=FALSE,results='asis'}
A9<- matrix(c(3,0.5,0.5,2),2,2)
B9<- matrix(c(2,-1,-1,1),2,2)
W9<- matrix(c(1,1,1,2),2,2)
```

Con $Y$ y $\sum$ como en el ejercicio anterior, establezca si las formas cuadráticas $Y´BY$ e $Y´AY$ son independientes, donde: $B=$
```{r echo=FALSE,results='asis'}
"$" %_% xm(B9, mtype = "bm") %_% "$"
```
y $A=$
```{r echo=FALSE,results='asis'}
"$" %_% xm(A9, mtype = "bm") %_% "$"
```

## Ejercicio 10

El estadístico $T$ para la prueba de hipótesis de diferencia de medias está dado por:
$$T=\frac{(\overline{X}_1-\overline{X}_2)-(\mu_1-\mu_2)}{\sqrt[2]{S_P^2(\frac{1}{n_1}+\frac{1}{n_2}) }}$$
a. Demuestre que este estadístico tiene distribución *t-Student* con $(n_1+n_2-2)$ grados de libertad cuando $(\overline{X}_1-\overline{X}_2)$ es obtenida a partír de muestras aleatorias simples e independientes de tamaños $n_1$ y $n_2$, de poblaciones normales con idéntica varianza ($\sigma^2$)y esperanzas $\mu_1$ y $\mu_2$.
b.  Demostrar que: $S^2_p={\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}}$ es un estimador insesgado de $\sigma^2$.

## Ejercicio 11

El estadístico $F$ para la prueba de hipótesis de igualdad de varianzas está dado por:
$$F= \frac{S_1^2/\sigma_1^2}{S_1^2/\sigma_1^2}$$
Demuestre que este estadístico tiene distribución F con $(n_1-n_2-1)$ grados de libertad cuando las muestras aleatorias son independientes de tamaños $n_1$ y $n_2$ provenientes de poblaciones normales





